import os
import re
import requests
import subprocess
import tempfile
import shutil
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
from googleapiclient.discovery import build
from openai import OpenAI
from rapidfuzz import fuzz
from PIL import Image

st.set_page_config(layout="wide")
st.title("ðŸ” Zero1 YouTube Title & Thumbnail Matcher")

# â”€â”€ Only search within these 83 channels â”€â”€
ALLOWED_CHANNELS = [
    "UCK7tptUDHh-RYDsdxO1-5QQ","UCvJJ_dzjViJCoLf5uKUTwoA","UCvQECJukTDE2i6aCoMnS-Vg",
    # â€¦ (all your other channel IDs) â€¦
    "UCczAxLCL79gHXKYaEc9k-ZQ","UCqykZoZjaOPb6i_Y5gk0kLQ"
]

# â”€â”€ Secrets & clients â”€â”€
YT_KEY     = st.secrets["YOUTUBE"]["API_KEY"]
OPENAI_KEY = st.secrets["OPENAI"]["API_KEY"]
VISION_KEY = st.secrets["VISION"]["API_KEY"]

youtube    = build("youtube", "v3", developerKey=YT_KEY)
openai_cli = OpenAI(api_key=OPENAI_KEY)

# â”€â”€ Helpers â”€â”€
def parse_iso_duration(dur: str) -> int:
    m = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", dur)
    return (int(m.group(1) or 0)*3600 +
            int(m.group(2) or 0)*60 +
            int(m.group(3) or 0))

def format_views(n: int) -> str:
    if n >= 1_000_000: return f"{n/1_000_000:.1f}M"
    if n >=   1_000: return f"{n/1_000:.1f}K"
    return str(n)

@st.cache_data
def fetch_my_videos(cid: str) -> list[str]:
    ids, req = [], youtube.search().list(
        part="id", channelId=cid, type="video",
        order="date", maxResults=50
    )
    while req:
        res = req.execute()
        ids += [item["id"]["videoId"] for item in res.get("items", [])]
        req = youtube.search().list_next(req, res)
    return ids

@st.cache_data
def fetch_video_details(vids: list[str]) -> pd.DataFrame:
    rows = []
    for i in range(0, len(vids), 50):
        chunk = vids[i:i+50]
        resp = youtube.videos().list(
            part="snippet,contentDetails,statistics",
            id=",".join(chunk)
        ).execute()
        for v in resp.get("items", []):
            sec = parse_iso_duration(v["contentDetails"]["duration"])
            pub = v["snippet"]["publishedAt"]
            rows.append({
                "videoId":    v["id"],
                "title":      v["snippet"]["title"],
                "type":       "Short" if sec <= 180 else "Long-Form",
                "channel":    v["snippet"]["channelTitle"],
                "uploadDate": datetime.fromisoformat(pub.rstrip("Z")).date().isoformat(),
                "thumb":      v["snippet"]["thumbnails"]["high"]["url"],
                "views":      int(v["statistics"].get("viewCount", 0))
            })
    return pd.DataFrame(rows)

@st.cache_data
def get_embedding(text: str) -> np.ndarray:
    resp = openai_cli.embeddings.create(model="text-embedding-ada-002", input=text)
    return np.array(resp.data[0].embedding, dtype=np.float32)

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float((a @ b) / (np.linalg.norm(a)*np.linalg.norm(b))) * 100.0

def extract_text_via_vision(url: str) -> str:
    r = requests.post(
        f"https://vision.googleapis.com/v1/images:annotate?key={VISION_KEY}",
        json={"requests":[{"image":{"source":{"imageUri":url}},
                           "features":[{"type":"TEXT_DETECTION","maxResults":1}]}]}
    ).json()
    try:
        return r["responses"][0]["fullTextAnnotation"]["text"]
    except:
        return ""

@st.cache_data(show_spinner=False)
def get_intro_text(video_id: str, seconds: int) -> str:
    tmpdir = tempfile.mkdtemp()
    try:
        out = os.path.join(tmpdir, f"{video_id}.webm")
        subprocess.run([
            "yt-dlp","-f","bestaudio",
            "--download-sections", f"*00:00:00-00:00:{seconds:02d}",
            "-o", out, f"https://youtu.be/{video_id}"
        ], check=True)
        with open(out,"rb") as f:
            resp = openai_cli.audio.transcriptions.create(model="whisper-1", file=f)
        return resp.text
    except subprocess.CalledProcessError:
        return ""
    finally:
        shutil.rmtree(tmpdir, ignore_errors=True)

# â”€â”€ Sidebar â”€â”€
channel_id   = st.sidebar.text_input("Your Channel ID")
content_type = st.sidebar.selectbox("Filter by:", ["Long-Form (>3 min)", "Shorts (â‰¤ 3 min)"])
num_matches  = st.sidebar.number_input("Results to show", 1, 10, 5)

if not channel_id:
    st.info("Enter your YouTube Channel ID."); st.stop()

with st.spinner("Loading your uploadsâ€¦"):
    my_ids = fetch_my_videos(channel_id)
if not my_ids:
    st.error("No videos found."); st.stop()

df_all = fetch_video_details(my_ids)
want_short = content_type.startswith("Shorts")
df        = df_all[df_all["type"] == ("Short" if want_short else "Long-Form")]
if df.empty:
    st.warning(f"No {content_type} found."); st.stop()

# â”€â”€ 1) Select & show your video â”€â”€
st.subheader("1) Select one of your videos")
sel = st.selectbox("Your videos", df["title"].tolist())
src = df[df["title"] == sel].iloc[0]
st.image(src["thumb"], caption=f"â–¶ï¸ {sel}", width=300)
st.markdown(
    f"**Channel:** {src['channel']}  "
    f"**Uploaded:** {src['uploadDate']}  "
    f"**Views:** {format_views(src['views'])}"
)

# â”€â”€ 2) Enter keyword â”€â”€
st.subheader("2) Enter a primary keyword (mandatory)")
pk = st.text_input("Primary keyword")
if not pk:
    st.info("Enter a primary keyword."); st.stop()

# â”€â”€ Precompute â”€â”€
emb_src  = get_embedding(src["title"])
text_src = extract_text_via_vision(src["thumb"])
img      = Image.open(requests.get(src["thumb"], stream=True).raw).convert("RGB").resize((256,256))
hist_src = img.histogram(); total = sum(hist_src)
def hist_sim(url: str) -> float:
    im2  = Image.open(requests.get(url, stream=True).raw).convert("RGB").resize((256,256))
    h2   = im2.histogram()
    inter = sum(min(hist_src[i],h2[i]) for i in range(len(h2)))
    return inter/total*100.0

# â”€â”€ 3) Run match â”€â”€
if st.button("3) Run Title, Thumbnail & Intro Match"):
    def yt_search(q):
        return requests.get(
            "https://youtube.googleapis.com/youtube/v3/search",
            params=dict(part="snippet",q=q,type="video",
                        order="viewCount",maxResults=50,key=YT_KEY)
        ).json().get("items", [])
    sem_items = yt_search(src["title"])
    key_items = yt_search(pk)
    cand_sem  = [it["id"]["videoId"] for it in sem_items if it["snippet"]["channelId"] in ALLOWED_CHANNELS]
    cand_key  = [it["id"]["videoId"] for it in key_items if it["snippet"]["channelId"] in ALLOWED_CHANNELS]
    combined  = list(dict.fromkeys(cand_sem + cand_key))
    if not combined:
        st.warning("No matches found."); st.stop()

    df_cand = fetch_video_details(combined)
    # Title metrics
    df_cand["Sem %"]      = df_cand["title"].map(lambda t: cosine_sim(emb_src, get_embedding(t)))
    df_cand["Key %"]      = df_cand["title"].map(lambda t: fuzz.ratio(pk, t))
    df_cand["Combined %"] = df_cand[["Sem %","Key %"]].max(axis=1)
    # Thumbnail metrics
    df_cand["Text %"]     = df_cand["thumb"].map(lambda u: fuzz.ratio(text_src, extract_text_via_vision(u)))
    df_cand["Visual %"]   = df_cand["thumb"].map(hist_sim)
    # Intro metrics
    secs  = 20 if want_short else 60
    intro = get_intro_text(src["videoId"], secs)
    if intro:
        df_cand["Introâ†’Title %"]     = df_cand["title"].map(lambda t: fuzz.ratio(intro, t))
        df_cand["Introâ†’ThumbText %"] = df_cand["thumb"].map(lambda u: fuzz.ratio(intro, extract_text_via_vision(u)))
        df_cand["Intro Combined %"]  = df_cand[["Introâ†’Title %","Introâ†’ThumbText %"]].max(axis=1)
    else:
        df_cand["Introâ†’Title %"]     = 0
        df_cand["Introâ†’ThumbText %"] = 0
        df_cand["Intro Combined %"]  = 0

    # â”€â”€ Summary â”€â”€
    top1_ids = set(df_cand.nlargest(num_matches, "Combined %")["videoId"])
    top2_ids = set(df_cand.nlargest(num_matches, ["Visual %","Text %"])["videoId"])
    top3_ids = set(df_cand.nlargest(num_matches, "Intro Combined %")["videoId"])
    summary_ids = list(top1_ids | top2_ids | top3_ids)

    st.subheader("Summary of Matches")
    md_sum  = "| Title | Titleâœ“ | Thumbâœ“ | Introâœ“ |\n| --- | :---: | :---: | :---: |\n"
    for vid in summary_ids:
        r    = df_cand[df_cand["videoId"]==vid].iloc[0]
        link = f"[{r['title']}](https://youtu.be/{vid})"
        md_sum += f"| {link} | {'âœ“' if vid in top1_ids else 'âœ—'} | {'âœ“' if vid in top2_ids else 'âœ—'} | {'âœ“' if vid in top3_ids else 'âœ—'} |\n"
    st.markdown(md_sum, unsafe_allow_html=True)

    # â”€â”€ Table 1: Title Matches â”€â”€
    st.subheader("Table 1 â€“ Title Matches")
    tab_long, tab_short = st.tabs(["Long-Form Matches", "Shorts Matches"])
    for tab, vid_type in [(tab_long, "Long-Form"), (tab_short, "Short")]:
        with tab:
            sub = df_cand[df_cand["type"] == vid_type]
            top = sub.nlargest(num_matches, "Combined %")
            md  = "| Title | Channel | Uploaded | Views | Sem % | Key % | Combined % |\n"
            md += "| --- | --- | --- | ---: | ---: | ---: | ---: |\n"
            for _, r in top.iterrows():
                md += (
                    f"| [{r['title']}](https://youtu.be/{r.videoId}) | {r['channel']} | "
                    f"{r['uploadDate']} | {format_views(r['views'])} | "
                    f"{r['Sem %']:.1f}% | {r['Key %']:.1f}% | {r['Combined %']:.1f}% |\n"
                )
            st.markdown(md, unsafe_allow_html=True)

    # â”€â”€ Table 2: Thumbnail Matches â”€â”€
    st.subheader("Table 2 â€“ Thumbnail Matches")
    tab_long2, tab_short2 = st.tabs(["Long-Form Matches", "Shorts Matches"])
    for tab, vid_type in [(tab_long2, "Long-Form"), (tab_short2, "Short")]:
        with tab:
            sub = df_cand[df_cand["type"] == vid_type]
            sub = sub[(sub["Text %"] > 0) | (sub["Visual %"] > 0)]
            top = sub.sort_values(["Visual %","Text %"], ascending=[False,False]).head(num_matches)
            md  = "| Thumbnail | Title | Channel | Uploaded | Views | Text % | Visual % |\n"
            md += "| :---: | --- | --- | :---: | ---: | ---: | ---: |\n"
            for _, r in top.iterrows():
                md += (
                    f"| ![]({r['thumb']}) | [{r['title']}](https://youtu.be/{r.videoId}) | {r['channel']} | "
                    f"{r['uploadDate']} | {format_views(r['views'])} | "
                    f"{r['Text %']:.1f}% | {r['Visual %']:.1f}% |\n"
                )
            st.markdown(md, unsafe_allow_html=True)

    # â”€â”€ Table 3: Intro Text Matches â”€â”€
    st.subheader("Table 3 â€“ Intro Text Matches")
    if not intro:
        st.warning("No audio transcript available.")
    else:
        tab_long3, tab_short3 = st.tabs(["Long-Form Matches", "Shorts Matches"])
        for tab, vid_type in [(tab_long3, "Long-Form"), (tab_short3, "Short")]:
            with tab:
                sub = df_cand[df_cand["type"] == vid_type]
                top = sub.nlargest(num_matches, "Intro Combined %")
                md  = "| Title | Channel | Uploaded | Views | Introâ†’Title % | Introâ†’ThumbText % | Combined % |\n"
                md += "| --- | --- | --- | ---: | ---: | ---: | ---: |\n"
                for _, r in top.iterrows():
                    md += (
                        f"| [{r['title']}](https://youtu.be/{r.videoId}) | {r['channel']} | "
                        f"{r['uploadDate']} | {format_views(r['views'])} | "
                        f"{r['Introâ†’Title %']:.1f}% | {r['Introâ†’ThumbText %']:.1f}% | {r['Intro Combined %']:.1f}% |\n"
                    )
                st.markdown(md, unsafe_allow_html=True)
